{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "num_clients = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antonio - Unibo\n",
    "# s0.95\n",
    "# lr0.5\n",
    "# lda1.0\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-18/20-20-39/working\" # pp\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-18/14-55-39/working\" # topk\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-18/14-56-02/working\" # swat\n",
    "\n",
    "# s0.9\n",
    "# lr0.5\n",
    "# lda1.0\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-18/20-20-39/working\" # pp -\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-18/14-55-39/working\" # topk -\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-24/09-28-49/working\" # swat\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-24/14-33-58/working\" # swat + pp\n",
    "\n",
    "# lr0.1\n",
    "# lda1.0\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-21/15-27-13/working\" # pp\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-24/07-28-12/working\" # pp\n",
    "# directory_path = \"/home/aguastella/fl_powerpropagation/outputs/2024-06-21/14-05-50/working\" # topk\n",
    "# directory_path =  \"/home/aguastella/fl_powerpropagation/outputs/2024-06-21/18-26-35/working\" # swat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeph\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-29/17-28-42/working\" # top-k\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-29/17-28-42/working/client_masks\" # top-k\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-29/18-23-34/working\" # top-k\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-29/18-23-34/working/client_masks\" # top-k\n",
    "# # lda0.1\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-30/12-54-09/working\" # top-k\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-30/12-54-09/working/client_masks\" # top-k\n",
    "# # before pruning\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-30/15-44-37/working\" # top-k\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-30/15-44-37/working/client_masks\" # top-k\n",
    "# fedAvgNZ\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-30/16-26-03/working\" # top-k\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-30/16-26-03/working/client_masks\" # top-k\n",
    "\n",
    "\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-29/18-02-37/working\" # pp\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-29/18-02-37/working/client_masks\" # pp\n",
    "# global_directory_path =   \"/home/zep/fl_powerpropagation/outputs/2024-07-29/18-30-23/working\" # pp\n",
    "# clients_directory_path =  \"/home/zep/fl_powerpropagation/outputs/2024-07-29/18-30-23/working/client_masks\" # pp\n",
    "# lda0.1\n",
    "# global_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-30/13-00-05/working\" # pp\n",
    "# clients_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-30/13-00-05/working/client_masks\" # pp\n",
    "# before pruning\n",
    "# global_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-30/15-51-15/working\" # pp\n",
    "# clients_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-30/15-51-15/working/client_masks\" # pp\n",
    "\n",
    "# SWAT\n",
    "# lda0.1\n",
    "# global_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-30/17-19-50/working\" # pp\n",
    "# clients_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-30/17-19-50/working/client_masks\" # pp\n",
    "# ZEROFL\n",
    "# lda1000.0\n",
    "# global_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-31/10-47-25/working\" # pp\n",
    "# clients_directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-07-31/10-47-25/working/client_masks\" # pp\n",
    "# lda0.1\n",
    "global_directory_path = (  # pp\n",
    "    \"/home/zep/fl_powerpropagation/outputs/2024-07-31/10-35-23/working\"\n",
    ")\n",
    "clients_directory_path = (  # pp\n",
    "    \"/home/zep/fl_powerpropagation/outputs/2024-07-31/10-35-23/working/client_masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import re\n",
    "\n",
    "\n",
    "sampling_rate = 1\n",
    "\n",
    "\n",
    "def extract_numbers(filename):\n",
    "    match = re.search(r\"mask_(\\d+)_client_(\\d+)\", filename)\n",
    "    if match:\n",
    "        mask_num = int(match.group(1))\n",
    "        client_num = int(match.group(2))\n",
    "        return mask_num, client_num\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# GLOBAL\n",
    "# Get a list of all pickle files\n",
    "global_pickle_files = glob.glob(f\"{global_directory_path}/mask_*.pickle\")\n",
    "# print(pickle_files)\n",
    "# sort by name\n",
    "global_pickle_files.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "# remove masks\n",
    "# global_pickle_files.pop(0)\n",
    "# pickle_files.pop(1)\n",
    "# keep only the first 300\n",
    "# pickle_files = pickle_files[:10]\n",
    "# keep only one overy 10\n",
    "print(\"Global masks: \", global_pickle_files)\n",
    "global_pickle_files = global_pickle_files[::sampling_rate]\n",
    "\n",
    "# Load the global masks\n",
    "global_masks = []\n",
    "for file in global_pickle_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        global_mask = pickle.load(f)\n",
    "        global_masks.append(global_mask)\n",
    "\n",
    "\n",
    "# CLIENTS\n",
    "clients_pickle_files = glob.glob(f\"{clients_directory_path}/mask_*.pickle\")\n",
    "clients_pickle_files = sorted(clients_pickle_files, key=lambda x: extract_numbers(x))\n",
    "# Remove the first mask that is form the test client\n",
    "clients_pickle_files.pop(0)\n",
    "print(\"Client masks: \", clients_pickle_files)\n",
    "\n",
    "\n",
    "# Load the client masks\n",
    "clients_masks = []\n",
    "for file in clients_pickle_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        client_mask = pickle.load(f)\n",
    "        clients_masks.append(client_mask)\n",
    "\n",
    "# Ensure each mask is a list of numpy arrays\n",
    "# masks = [[np.array(layer_mask) for layer_mask in mask] for mask in masks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reordering layer of the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume original_ordered_keys and sorted_ordered_keys are given\n",
    "original_ordered_keys = {\n",
    "    0: \"net.conv1.weight\",\n",
    "    1: \"net.bn1.weight\",\n",
    "    2: \"net.bn1.bias\",\n",
    "    3: \"net.bn1.running_mean\",\n",
    "    4: \"net.bn1.running_var\",\n",
    "    5: \"net.bn1.num_batches_tracked\",\n",
    "    6: \"net.layer1.0.conv1.weight\",\n",
    "    7: \"net.layer1.0.bn1.weight\",\n",
    "    8: \"net.layer1.0.bn1.bias\",\n",
    "    9: \"net.layer1.0.bn1.running_mean\",\n",
    "    10: \"net.layer1.0.bn1.running_var\",\n",
    "    11: \"net.layer1.0.bn1.num_batches_tracked\",\n",
    "    12: \"net.layer1.0.conv2.weight\",\n",
    "    13: \"net.layer1.0.bn2.weight\",\n",
    "    14: \"net.layer1.0.bn2.bias\",\n",
    "    15: \"net.layer1.0.bn2.running_mean\",\n",
    "    16: \"net.layer1.0.bn2.running_var\",\n",
    "    17: \"net.layer1.0.bn2.num_batches_tracked\",\n",
    "    18: \"net.layer1.1.conv1.weight\",\n",
    "    19: \"net.layer1.1.bn1.weight\",\n",
    "    20: \"net.layer1.1.bn1.bias\",\n",
    "    21: \"net.layer1.1.bn1.running_mean\",\n",
    "    22: \"net.layer1.1.bn1.running_var\",\n",
    "    23: \"net.layer1.1.bn1.num_batches_tracked\",\n",
    "    24: \"net.layer1.1.conv2.weight\",\n",
    "    25: \"net.layer1.1.bn2.weight\",\n",
    "    26: \"net.layer1.1.bn2.bias\",\n",
    "    27: \"net.layer1.1.bn2.running_mean\",\n",
    "    28: \"net.layer1.1.bn2.running_var\",\n",
    "    29: \"net.layer1.1.bn2.num_batches_tracked\",\n",
    "    30: \"net.layer2.0.conv1.weight\",\n",
    "    31: \"net.layer2.0.bn1.weight\",\n",
    "    32: \"net.layer2.0.bn1.bias\",\n",
    "    33: \"net.layer2.0.bn1.running_mean\",\n",
    "    34: \"net.layer2.0.bn1.running_var\",\n",
    "    35: \"net.layer2.0.bn1.num_batches_tracked\",\n",
    "    36: \"net.layer2.0.conv2.weight\",\n",
    "    37: \"net.layer2.0.bn2.weight\",\n",
    "    38: \"net.layer2.0.bn2.bias\",\n",
    "    39: \"net.layer2.0.bn2.running_mean\",\n",
    "    40: \"net.layer2.0.bn2.running_var\",\n",
    "    41: \"net.layer2.0.bn2.num_batches_tracked\",\n",
    "    42: \"net.layer2.0.downsample.0.weight\",\n",
    "    43: \"net.layer2.0.downsample.1.weight\",\n",
    "    44: \"net.layer2.0.downsample.1.bias\",\n",
    "    45: \"net.layer2.0.downsample.1.running_mean\",\n",
    "    46: \"net.layer2.0.downsample.1.running_var\",\n",
    "    47: \"net.layer2.0.downsample.1.num_batches_tracked\",\n",
    "    48: \"net.layer2.1.conv1.weight\",\n",
    "    49: \"net.layer2.1.bn1.weight\",\n",
    "    50: \"net.layer2.1.bn1.bias\",\n",
    "    51: \"net.layer2.1.bn1.running_mean\",\n",
    "    52: \"net.layer2.1.bn1.running_var\",\n",
    "    53: \"net.layer2.1.bn1.num_batches_tracked\",\n",
    "    54: \"net.layer2.1.conv2.weight\",\n",
    "    55: \"net.layer2.1.bn2.weight\",\n",
    "    56: \"net.layer2.1.bn2.bias\",\n",
    "    57: \"net.layer2.1.bn2.running_mean\",\n",
    "    58: \"net.layer2.1.bn2.running_var\",\n",
    "    59: \"net.layer2.1.bn2.num_batches_tracked\",\n",
    "    60: \"net.layer3.0.conv1.weight\",\n",
    "    61: \"net.layer3.0.bn1.weight\",\n",
    "    62: \"net.layer3.0.bn1.bias\",\n",
    "    63: \"net.layer3.0.bn1.running_mean\",\n",
    "    64: \"net.layer3.0.bn1.running_var\",\n",
    "    65: \"net.layer3.0.bn1.num_batches_tracked\",\n",
    "    66: \"net.layer3.0.conv2.weight\",\n",
    "    67: \"net.layer3.0.bn2.weight\",\n",
    "    68: \"net.layer3.0.bn2.bias\",\n",
    "    69: \"net.layer3.0.bn2.running_mean\",\n",
    "    70: \"net.layer3.0.bn2.running_var\",\n",
    "    71: \"net.layer3.0.bn2.num_batches_tracked\",\n",
    "    72: \"net.layer3.0.downsample.0.weight\",\n",
    "    73: \"net.layer3.0.downsample.1.weight\",\n",
    "    74: \"net.layer3.0.downsample.1.bias\",\n",
    "    75: \"net.layer3.0.downsample.1.running_mean\",\n",
    "    76: \"net.layer3.0.downsample.1.running_var\",\n",
    "    77: \"net.layer3.0.downsample.1.num_batches_tracked\",\n",
    "    78: \"net.layer3.1.conv1.weight\",\n",
    "    79: \"net.layer3.1.bn1.weight\",\n",
    "    80: \"net.layer3.1.bn1.bias\",\n",
    "    81: \"net.layer3.1.bn1.running_mean\",\n",
    "    82: \"net.layer3.1.bn1.running_var\",\n",
    "    83: \"net.layer3.1.bn1.num_batches_tracked\",\n",
    "    84: \"net.layer3.1.conv2.weight\",\n",
    "    85: \"net.layer3.1.bn2.weight\",\n",
    "    86: \"net.layer3.1.bn2.bias\",\n",
    "    87: \"net.layer3.1.bn2.running_mean\",\n",
    "    88: \"net.layer3.1.bn2.running_var\",\n",
    "    89: \"net.layer3.1.bn2.num_batches_tracked\",\n",
    "    90: \"net.layer4.0.conv1.weight\",\n",
    "    91: \"net.layer4.0.bn1.weight\",\n",
    "    92: \"net.layer4.0.bn1.bias\",\n",
    "    93: \"net.layer4.0.bn1.running_mean\",\n",
    "    94: \"net.layer4.0.bn1.running_var\",\n",
    "    95: \"net.layer4.0.bn1.num_batches_tracked\",\n",
    "    96: \"net.layer4.0.conv2.weight\",\n",
    "    97: \"net.layer4.0.bn2.weight\",\n",
    "    98: \"net.layer4.0.bn2.bias\",\n",
    "    99: \"net.layer4.0.bn2.running_mean\",\n",
    "    100: \"net.layer4.0.bn2.running_var\",\n",
    "    101: \"net.layer4.0.bn2.num_batches_tracked\",\n",
    "    102: \"net.layer4.0.downsample.0.weight\",\n",
    "    103: \"net.layer4.0.downsample.1.weight\",\n",
    "    104: \"net.layer4.0.downsample.1.bias\",\n",
    "    105: \"net.layer4.0.downsample.1.running_mean\",\n",
    "    106: \"net.layer4.0.downsample.1.running_var\",\n",
    "    107: \"net.layer4.0.downsample.1.num_batches_tracked\",\n",
    "    108: \"net.layer4.1.conv1.weight\",\n",
    "    109: \"net.layer4.1.bn1.weight\",\n",
    "    110: \"net.layer4.1.bn1.bias\",\n",
    "    111: \"net.layer4.1.bn1.running_mean\",\n",
    "    112: \"net.layer4.1.bn1.running_var\",\n",
    "    113: \"net.layer4.1.bn1.num_batches_tracked\",\n",
    "    114: \"net.layer4.1.conv2.weight\",\n",
    "    115: \"net.layer4.1.bn2.weight\",\n",
    "    116: \"net.layer4.1.bn2.bias\",\n",
    "    117: \"net.layer4.1.bn2.running_mean\",\n",
    "    118: \"net.layer4.1.bn2.running_var\",\n",
    "    119: \"net.layer4.1.bn2.num_batches_tracked\",\n",
    "    120: \"net.fc.weight\",\n",
    "    121: \"net.fc.bias\",\n",
    "}\n",
    "sorted_ordered_keys = {\n",
    "    0: \"net.bn1.bias\",\n",
    "    1: \"net.bn1.num_batches_tracked\",\n",
    "    2: \"net.bn1.running_mean\",\n",
    "    3: \"net.bn1.running_var\",\n",
    "    4: \"net.bn1.weight\",\n",
    "    5: \"net.conv1.weight\",\n",
    "    6: \"net.fc.bias\",\n",
    "    7: \"net.fc.weight\",\n",
    "    8: \"net.layer1.0.bn1.bias\",\n",
    "    9: \"net.layer1.0.bn1.num_batches_tracked\",\n",
    "    10: \"net.layer1.0.bn1.running_mean\",\n",
    "    11: \"net.layer1.0.bn1.running_var\",\n",
    "    12: \"net.layer1.0.bn1.weight\",\n",
    "    13: \"net.layer1.0.bn2.bias\",\n",
    "    14: \"net.layer1.0.bn2.num_batches_tracked\",\n",
    "    15: \"net.layer1.0.bn2.running_mean\",\n",
    "    16: \"net.layer1.0.bn2.running_var\",\n",
    "    17: \"net.layer1.0.bn2.weight\",\n",
    "    18: \"net.layer1.0.conv1.weight\",\n",
    "    19: \"net.layer1.0.conv2.weight\",\n",
    "    20: \"net.layer1.1.bn1.bias\",\n",
    "    21: \"net.layer1.1.bn1.num_batches_tracked\",\n",
    "    22: \"net.layer1.1.bn1.running_mean\",\n",
    "    23: \"net.layer1.1.bn1.running_var\",\n",
    "    24: \"net.layer1.1.bn1.weight\",\n",
    "    25: \"net.layer1.1.bn2.bias\",\n",
    "    26: \"net.layer1.1.bn2.num_batches_tracked\",\n",
    "    27: \"net.layer1.1.bn2.running_mean\",\n",
    "    28: \"net.layer1.1.bn2.running_var\",\n",
    "    29: \"net.layer1.1.bn2.weight\",\n",
    "    30: \"net.layer1.1.conv1.weight\",\n",
    "    31: \"net.layer1.1.conv2.weight\",\n",
    "    32: \"net.layer2.0.bn1.bias\",\n",
    "    33: \"net.layer2.0.bn1.num_batches_tracked\",\n",
    "    34: \"net.layer2.0.bn1.running_mean\",\n",
    "    35: \"net.layer2.0.bn1.running_var\",\n",
    "    36: \"net.layer2.0.bn1.weight\",\n",
    "    37: \"net.layer2.0.bn2.bias\",\n",
    "    38: \"net.layer2.0.bn2.num_batches_tracked\",\n",
    "    39: \"net.layer2.0.bn2.running_mean\",\n",
    "    40: \"net.layer2.0.bn2.running_var\",\n",
    "    41: \"net.layer2.0.bn2.weight\",\n",
    "    42: \"net.layer2.0.conv1.weight\",\n",
    "    43: \"net.layer2.0.conv2.weight\",\n",
    "    44: \"net.layer2.0.downsample.0.weight\",\n",
    "    45: \"net.layer2.0.downsample.1.bias\",\n",
    "    46: \"net.layer2.0.downsample.1.num_batches_tracked\",\n",
    "    47: \"net.layer2.0.downsample.1.running_mean\",\n",
    "    48: \"net.layer2.0.downsample.1.running_var\",\n",
    "    49: \"net.layer2.0.downsample.1.weight\",\n",
    "    50: \"net.layer2.1.bn1.bias\",\n",
    "    51: \"net.layer2.1.bn1.num_batches_tracked\",\n",
    "    52: \"net.layer2.1.bn1.running_mean\",\n",
    "    53: \"net.layer2.1.bn1.running_var\",\n",
    "    54: \"net.layer2.1.bn1.weight\",\n",
    "    55: \"net.layer2.1.bn2.bias\",\n",
    "    56: \"net.layer2.1.bn2.num_batches_tracked\",\n",
    "    57: \"net.layer2.1.bn2.running_mean\",\n",
    "    58: \"net.layer2.1.bn2.running_var\",\n",
    "    59: \"net.layer2.1.bn2.weight\",\n",
    "    60: \"net.layer2.1.conv1.weight\",\n",
    "    61: \"net.layer2.1.conv2.weight\",\n",
    "    62: \"net.layer3.0.bn1.bias\",\n",
    "    63: \"net.layer3.0.bn1.num_batches_tracked\",\n",
    "    64: \"net.layer3.0.bn1.running_mean\",\n",
    "    65: \"net.layer3.0.bn1.running_var\",\n",
    "    66: \"net.layer3.0.bn1.weight\",\n",
    "    67: \"net.layer3.0.bn2.bias\",\n",
    "    68: \"net.layer3.0.bn2.num_batches_tracked\",\n",
    "    69: \"net.layer3.0.bn2.running_mean\",\n",
    "    70: \"net.layer3.0.bn2.running_var\",\n",
    "    71: \"net.layer3.0.bn2.weight\",\n",
    "    72: \"net.layer3.0.conv1.weight\",\n",
    "    73: \"net.layer3.0.conv2.weight\",\n",
    "    74: \"net.layer3.0.downsample.0.weight\",\n",
    "    75: \"net.layer3.0.downsample.1.bias\",\n",
    "    76: \"net.layer3.0.downsample.1.num_batches_tracked\",\n",
    "    77: \"net.layer3.0.downsample.1.running_mean\",\n",
    "    78: \"net.layer3.0.downsample.1.running_var\",\n",
    "    79: \"net.layer3.0.downsample.1.weight\",\n",
    "    80: \"net.layer3.1.bn1.bias\",\n",
    "    81: \"net.layer3.1.bn1.num_batches_tracked\",\n",
    "    82: \"net.layer3.1.bn1.running_mean\",\n",
    "    83: \"net.layer3.1.bn1.running_var\",\n",
    "    84: \"net.layer3.1.bn1.weight\",\n",
    "    85: \"net.layer3.1.bn2.bias\",\n",
    "    86: \"net.layer3.1.bn2.num_batches_tracked\",\n",
    "    87: \"net.layer3.1.bn2.running_mean\",\n",
    "    88: \"net.layer3.1.bn2.running_var\",\n",
    "    89: \"net.layer3.1.bn2.weight\",\n",
    "    90: \"net.layer3.1.conv1.weight\",\n",
    "    91: \"net.layer3.1.conv2.weight\",\n",
    "    92: \"net.layer4.0.bn1.bias\",\n",
    "    93: \"net.layer4.0.bn1.num_batches_tracked\",\n",
    "    94: \"net.layer4.0.bn1.running_mean\",\n",
    "    95: \"net.layer4.0.bn1.running_var\",\n",
    "    96: \"net.layer4.0.bn1.weight\",\n",
    "    97: \"net.layer4.0.bn2.bias\",\n",
    "    98: \"net.layer4.0.bn2.num_batches_tracked\",\n",
    "    99: \"net.layer4.0.bn2.running_mean\",\n",
    "    100: \"net.layer4.0.bn2.running_var\",\n",
    "    101: \"net.layer4.0.bn2.weight\",\n",
    "    102: \"net.layer4.0.conv1.weight\",\n",
    "    103: \"net.layer4.0.conv2.weight\",\n",
    "    104: \"net.layer4.0.downsample.0.weight\",\n",
    "    105: \"net.layer4.0.downsample.1.bias\",\n",
    "    106: \"net.layer4.0.downsample.1.num_batches_tracked\",\n",
    "    107: \"net.layer4.0.downsample.1.running_mean\",\n",
    "    108: \"net.layer4.0.downsample.1.running_var\",\n",
    "    109: \"net.layer4.0.downsample.1.weight\",\n",
    "    110: \"net.layer4.1.bn1.bias\",\n",
    "    111: \"net.layer4.1.bn1.num_batches_tracked\",\n",
    "    112: \"net.layer4.1.bn1.running_mean\",\n",
    "    113: \"net.layer4.1.bn1.running_var\",\n",
    "    114: \"net.layer4.1.bn1.weight\",\n",
    "    115: \"net.layer4.1.bn2.bias\",\n",
    "    116: \"net.layer4.1.bn2.num_batches_tracked\",\n",
    "    117: \"net.layer4.1.bn2.running_mean\",\n",
    "    118: \"net.layer4.1.bn2.running_var\",\n",
    "    119: \"net.layer4.1.bn2.weight\",\n",
    "    120: \"net.layer4.1.conv1.weight\",\n",
    "    121: \"net.layer4.1.conv2.weight\",\n",
    "}\n",
    "\n",
    "# Create a mapping from sorted order to original order\n",
    "index_mapping = {v: k for k, v in original_ordered_keys.items()}\n",
    "sorted_indices = [\n",
    "    index_mapping[sorted_ordered_keys[i]] for i in range(len(sorted_ordered_keys))\n",
    "]\n",
    "\n",
    "\n",
    "# Function to reorder masks\n",
    "def reorder_masks(mask_list):\n",
    "    reordered_mask_list = []\n",
    "    for round_masks in mask_list:\n",
    "        reordered_masks = [None] * len(round_masks)\n",
    "        for i, sorted_index in enumerate(sorted_indices):\n",
    "            reordered_masks[sorted_index] = round_masks[i]\n",
    "        reordered_mask_list.append(reordered_masks)\n",
    "    return reordered_mask_list\n",
    "\n",
    "\n",
    "# masks = reorder_masks(masks)\n",
    "global_masks = reorder_masks(global_masks)\n",
    "clients_masks = reorder_masks(clients_masks)\n",
    "\n",
    "\n",
    "# Printing the reordered masks for verification\n",
    "# for i, round_masks in enumerate(reordered_masks):\n",
    "#     print(f\"Round {i+1} reordered masks:\")\n",
    "#     for j, mask in enumerate(round_masks):\n",
    "#         print(f\"Layer {j}: {mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mask in masks:\n",
    "#     for m in mask:\n",
    "#         if m.shape != (10, ) and m.shape != (64, ) and m.shape != () and m.shape != (128,) and m.shape != (256,) and m.shape != (512,):\n",
    "#             print(m.shape)\n",
    "#             # print the type of mask\n",
    "#     # print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory containing the .bin files\n",
    "# directory_path = \"/home/zep/fl_powerpropagation/outputs/2024-06-11/18-26-01/results/parameters\"\n",
    "\n",
    "# # Get a list of all .bin files\n",
    "# bin_files = sorted(glob.glob(f\"{directory_path}/parameters*.bin\"))\n",
    "\n",
    "# # Load the model parameters\n",
    "# models = []\n",
    "# for file in bin_files:\n",
    "#     try:\n",
    "#         # Attempt to load using torch.load\n",
    "#         model_state = torch.load(file)\n",
    "#         models.append(model_state)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# # Convert model parameters to binary masks\n",
    "# masks = []\n",
    "# for model_state in models:\n",
    "#     mask = [param != 0 for param in model_state.values()]\n",
    "#     masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure each mask is a list of numpy arrays\n",
    "# masks = [[np.array(layer_mask) for layer_mask in mask] for mask in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute overlap percentage between two masks\n",
    "# def compute_overlap_percentage(mask1, mask2):\n",
    "#     total_weights = sum(m1.size for m1 in mask1)  # Total number of weights\n",
    "#     overlap_weights = sum(\n",
    "#         np.sum(m1 & m2) for m1, m2 in zip(mask1, mask2)\n",
    "#     )  # Overlapping weights\n",
    "#     return (overlap_weights / total_weights) * 100  # Percentage of overlap\n",
    "\n",
    "\n",
    "def intersection_over_union(mask1, mask2):\n",
    "    intersection = sum(np.sum(m1 & m2) for m1, m2 in zip(mask1, mask2))\n",
    "    union = sum(np.sum(m1 | m2) for m1, m2 in zip(mask1, mask2))\n",
    "    # print(intersection, union)\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "# Compute overlap percentage between consecutive masks\n",
    "# overlap_percentages = [\n",
    "#     intersection_over_union(global_masks[i], global_masks[i + 1]) for i in range(len(global_masks) - 1)\n",
    "# ]\n",
    "\n",
    "# Compute the overlap percentage between the global masks and the client masks\n",
    "# Each global mask is compared to each client mask for the same round (10 masks for round)\n",
    "# overlap_percentages = [intersection_over_union(global_masks[i], clients_masks[j]) for i in range(len(global_masks)-1) for j in range(num_clients)]\n",
    "overlap_percentages = []\n",
    "for i in range(len(global_masks) - 1):\n",
    "    for j in range(i * num_clients, i * num_clients + num_clients):\n",
    "        # print(f\"i: {i}, j: {j}\")\n",
    "        overlap_percentages.append(\n",
    "            intersection_over_union(global_masks[i], clients_masks[j])\n",
    "        )\n",
    "        print(f\"Round {i+1} Client {j+1}:\")\n",
    "        print(f\"Masks paths: {(global_pickle_files[i], clients_pickle_files[j])}\")\n",
    "        print(\n",
    "            \"IoU global vs client:\"\n",
    "            f\" {intersection_over_union(global_masks[i], clients_masks[j])}\"\n",
    "        )\n",
    "        # print(f\"IoU consecutive clients: {intersection_over_union(clients_masks[j], clients_masks[j+1])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(overlap_percentages)\n",
    "\n",
    "# Plot overlap percentage over time\n",
    "rounds = range(1, len(overlap_percentages) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rounds, overlap_percentages, marker=\"o\")\n",
    "plt.xlabel(\"Training Round\")\n",
    "plt.ylabel(\"Overlap Percentage\")\n",
    "plt.title(\"Overlap Percentage of Weights During Training\")\n",
    "for i in range(1, len(overlap_percentages) // 10):\n",
    "    plt.axvline(x=i * 10, color=\"grey\", linestyle=\"--\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.0)  # Set the y-axis limits\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "masks = clients_masks\n",
    "\n",
    "# Compute overlap percentage matrix\n",
    "num_masks = len(masks)\n",
    "overlap_matrix = np.zeros((num_masks, num_masks))\n",
    "overlap_matrix_layer = np.zeros((len(masks[0]), num_masks, num_masks))\n",
    "\n",
    "\n",
    "def compute_overlap(i, j):\n",
    "    # overlap_matrix[i, j] = compute_overlap_percentage(masks[i], masks[j])\n",
    "    overlap_matrix[i, j] = intersection_over_union(masks[i], masks[j])\n",
    "\n",
    "    # for k in range(len(masks[0])):\n",
    "    #     if masks[i][k].shape != (10, ) and masks[i][k].shape != (64, ) and masks[i][k].shape != () and masks[i][k].shape != (128,) and masks[i][k].shape != (256,) and masks[i][k].shape != (512,):\n",
    "    #         overlap_matrix_layer[k, i, j] = intersection_over_union(masks[i][k], masks[j][k])\n",
    "\n",
    "\n",
    "# Use multi-threading to parallelize the computation\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     futures = []\n",
    "#     for i in range(num_masks):\n",
    "#         for j in range(num_masks):\n",
    "#             futures.append(executor.submit(compute_overlap, i, j))\n",
    "#     # Wait for all computations to complete\n",
    "#     concurrent.futures.wait(futures)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in range(num_masks):\n",
    "        for j in range(i):\n",
    "            futures.append(executor.submit(compute_overlap, i, j))\n",
    "    # Wait for all computations to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Set the diagonal to 1.0\n",
    "np.fill_diagonal(overlap_matrix, 1.0)\n",
    "for i in range(num_masks):\n",
    "    for j in range(i):\n",
    "        overlap_matrix[j, i] = overlap_matrix[i, j]\n",
    "\n",
    "\n",
    "# Print the overlap matrix\n",
    "print(overlap_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    overlap_matrix,\n",
    "    annot=False,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=[\n",
    "        label * sampling_rate if label % 5 == 0 else \"\" for label in range(num_masks)\n",
    "    ],\n",
    "    yticklabels=[\n",
    "        label * sampling_rate if label % 5 == 0 else \"\" for label in range(num_masks)\n",
    "    ],\n",
    "    vmin=0,\n",
    "    vmax=1.0,\n",
    ")\n",
    "plt.xlabel(\"Rounds\")\n",
    "# plt.ylabel(\"Rounds\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import multiprocessing\n",
    "# import numpy as np\n",
    "# import ctypes\n",
    "# from multiprocessing import sharedctypes\n",
    "\n",
    "\n",
    "# def intersection_over_union(mask1, mask2):\n",
    "#     # Dummy implementation\n",
    "#     intersection = np.minimum(mask1, mask2).sum()\n",
    "#     union = np.maximum(mask1, mask2).sum()\n",
    "#     return intersection / union\n",
    "\n",
    "# # Compute overlap percentage matrix\n",
    "# num_masks = len(masks)\n",
    "# overlap_matrix = np.zeros((num_masks, num_masks))\n",
    "\n",
    "# # Create shared memory arrays\n",
    "# shared_array_base = sharedctypes.RawArray(ctypes.c_double, num_masks * num_masks * len(masks[0]))\n",
    "# overlap_matrix_layer = np.ctypeslib.as_array(shared_array_base)\n",
    "# overlap_matrix_layer = overlap_matrix_layer.reshape(len(masks[0]), num_masks, num_masks)\n",
    "\n",
    "# def compute_overlap_for_batch(batch, masks):\n",
    "#     local_overlap_matrix_layer = np.zeros((len(masks[0]), num_masks, num_masks))\n",
    "#     for i, j in batch:\n",
    "#         for k in range(len(masks[0])):\n",
    "#             if (\n",
    "#                 masks[i][k].shape != (10,)\n",
    "#                 and masks[i][k].shape != (64,)\n",
    "#                 and masks[i][k].shape != ()\n",
    "#                 and masks[i][k].shape != (128,)\n",
    "#                 and masks[i][k].shape != (256,)\n",
    "#                 and masks[i][k].shape != (512,)\n",
    "#             ):\n",
    "#                 local_overlap_matrix_layer[k, i, j] = intersection_over_union(\n",
    "#                     masks[i][k], masks[j][k]\n",
    "#                 )\n",
    "#     return local_overlap_matrix_layer\n",
    "\n",
    "# def parallel_computation(num_masks, masks, batch_size=10):\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "#         futures = []\n",
    "#         # Create batches of (i, j) pairs\n",
    "#         batches = [(i, j) for i in range(num_masks) for j in range(i)]\n",
    "#         for batch_start in range(0, len(batches), batch_size):\n",
    "#             batch = batches[batch_start:batch_start + batch_size]\n",
    "#             futures.append(executor.submit(compute_overlap_for_batch, batch, masks))\n",
    "\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             local_overlap_matrix_layer = future.result()\n",
    "#             overlap_matrix_layer += local_overlap_matrix_layer\n",
    "\n",
    "#     # Set the diagonal of overlap_matrix_layer to 1.0\n",
    "#     for k in range(len(masks[0])):\n",
    "#         np.fill_diagonal(overlap_matrix_layer[k], 1.0)\n",
    "\n",
    "#     # Fill the second half of the matrix\n",
    "#     for i in range(num_masks):\n",
    "#         for j in range(i):\n",
    "#             overlap_matrix_layer[:, j, i] = overlap_matrix_layer[:, i, j]\n",
    "\n",
    "#     return overlap_matrix_layer\n",
    "\n",
    "# # Execute the parallel computation\n",
    "# overlap_matrix_layer = parallel_computation(num_masks, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "# if m.shape != (10, ) and m.shape != (64, ) and m.shape != () and m.shape != (128,) and m.shape != (256,) and m.shape != (512,):\n",
    "\n",
    "# Compute overlap percentage matrix\n",
    "num_masks = len(masks)\n",
    "overlap_matrix = np.zeros((num_masks, num_masks))\n",
    "overlap_matrix_layer = np.zeros((len(masks[0]), num_masks, num_masks))\n",
    "\n",
    "\n",
    "def intersection_over_union(mask1, mask2):\n",
    "    # Vectorized implementation of intersection over union\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def compute_overlap(i, j):\n",
    "    # overlap_matrix[i, j] = compute_overlap_percentage(masks[i], masks[j])\n",
    "    # overlap_matrix[i, j] = intersection_over_union(masks[i], masks[j])\n",
    "\n",
    "    for k in range(len(masks[0])):\n",
    "        if (\n",
    "            masks[i][k].shape != (10,)\n",
    "            and masks[i][k].shape != (64,)\n",
    "            and masks[i][k].shape != ()\n",
    "            and masks[i][k].shape != (128,)\n",
    "            and masks[i][k].shape != (256,)\n",
    "            and masks[i][k].shape != (512,)\n",
    "        ):\n",
    "            overlap_matrix_layer[k, i, j] = intersection_over_union(\n",
    "                masks[i][k], masks[j][k]\n",
    "            )\n",
    "\n",
    "\n",
    "# Use multi-threading to parallelize the computation\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in range(num_masks):\n",
    "        for j in range(i):\n",
    "            futures.append(executor.submit(compute_overlap, i, j))\n",
    "    # Wait for all computations to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "\n",
    "# Set the diagonal of overlap_matrix_layer to 1.0\n",
    "for i in range(len(masks[0])):\n",
    "    np.fill_diagonal(overlap_matrix_layer[i], 1.0)\n",
    "# Fill the second half of the matrix\n",
    "for i in range(num_masks):\n",
    "    for j in range(i):\n",
    "        overlap_matrix_layer[:, j, i] = overlap_matrix_layer[:, i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS FOR EACH LAYER, DIVIDED\n",
    "\n",
    "# for i in range(len(masks[0])):\n",
    "#     if masks[0][i].shape != (10, ) and masks[0][i].shape != (64, ) and masks[0][i].shape != () and masks[0][i].shape != (128,) and masks[0][i].shape != (256,) and masks[0][i].shape != (512,):\n",
    "#         # Plot heatmap\n",
    "#         plt.figure(figsize=(5,4))\n",
    "#         sns.heatmap(\n",
    "#         overlap_matrix_layer[i],\n",
    "#         annot=False,\n",
    "#         fmt=\".2f\",\n",
    "#         cmap=\"viridis\",\n",
    "#         xticklabels='',\n",
    "#         # xticklabels=range(num_masks),\n",
    "#         yticklabels='',\n",
    "#         # yticklabels=range(num_masks),\n",
    "#         )\n",
    "#         # plt.xlabel(\"Round\")\n",
    "#         # plt.ylabel(\"Round\")\n",
    "#         plt.title(f\"Layer {i}, shape {masks[0][i].shape}\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# # # Plot heatmap\n",
    "# # plt.figure(figsize=(3,3))\n",
    "# # sns.heatmap(\n",
    "# # overlap_matrix_layer[0],\n",
    "# # annot=False,\n",
    "# # fmt=\".2f\",\n",
    "# # cmap=\"viridis\",\n",
    "# # xticklabels=range(num_masks),\n",
    "# # yticklabels=range(num_masks),\n",
    "# # )\n",
    "# # # plt.xlabel(\"Round\")\n",
    "# # # plt.ylabel(\"Round\")\n",
    "# # plt.tight_layout()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# Assuming overlap_matrix_layer and masks are defined earlier in the code\n",
    "\n",
    "# Determine the number of heatmaps to plot\n",
    "num_heatmaps = sum(\n",
    "    1\n",
    "    for i in range(len(masks[0]))\n",
    "    if masks[0][i].shape not in [(10,), (64,), (), (128,), (256,), (512,)]\n",
    ")\n",
    "\n",
    "# Calculate the grid size for subplots\n",
    "cols = 3  # You can change this to the desired number of columns\n",
    "rows = int(np.ceil(num_heatmaps / cols))\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each heatmap\n",
    "plot_index = 0\n",
    "tot_nz = np.sum(m.size for m in masks[-1])\n",
    "print(f\"Total elements in the last mask: {tot_nz}\")\n",
    "\n",
    "tot_layer_nz = 0\n",
    "for i in range(len(masks[0])):\n",
    "    if masks[0][i].shape not in [(10,), (64,), (), (128,), (256,), (512,)]:\n",
    "        # count the number of non-zero elements of a mask\n",
    "        tot_layer_nz += round(masks[-1][i].size / tot_nz * 100, 2)\n",
    "        sns.heatmap(\n",
    "            overlap_matrix_layer[i],\n",
    "            annot=False,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"viridis\",\n",
    "            xticklabels=\"\",\n",
    "            yticklabels=\"\",\n",
    "            vmin=0,\n",
    "            vmax=1.0,\n",
    "            ax=axes[plot_index],\n",
    "        )\n",
    "        axes[plot_index].set_title(\n",
    "            f\"Layer{i}, size:{masks[0][i].size},\"\n",
    "            f\" importance:{round(masks[-1][i].size/tot_nz*100, 2)}%,\"\n",
    "            f\" nz:{round(np.sum(masks[-1][i])/masks[0][i].size*100, 2)}%\"\n",
    "        )\n",
    "        plot_index += 1\n",
    "\n",
    "print(f\"Total non-zero elements in the last mask: {tot_layer_nz}\")\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(plot_index, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-a6kP9-Cp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
